{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding custom decision tree algorithm (modified CART)\n",
    "## Part 3: Splitting nodes\n",
    "\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This part will be a bit more \"technical\" than the previous two. We will continue building the tree bottom-up. I will discuss three new pieces of code in the following three sections. \n",
    "\n",
    "Our first step will be to find the optimal value on which to split a given feature. This can be realized in multiple ways, and I think the way we'll do it is pretty neat, but may not be the best one. This is something that can be optimized to have shorter training time. I will define two functions to achieve this goal.\n",
    "\n",
    "Next we will have to choose the feature on which to make a split - this will call on the first two functions. We will wrap it all together into a function that splits a \"node\" of the tree by choosing the best feature and value for the split.\n",
    "\n",
    "We will be using the ```get_gini_impurity``` function defined in Part 2. I'll show it again here for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute gini coefficient of an array, with given labels\n",
    "def get_gini_impurity(array, labels_list):\n",
    "    '''Computes Gini impurity of the given array of labels. \n",
    "    array [np.array] is the array for which Gini is computed.\n",
    "    labels_list [np.array] is the list of labels in the array.'''\n",
    "    label_probabilities = []\n",
    "    array_length = len(array)\n",
    "    if array_length==0:\n",
    "        return 0\n",
    "    #if array length is not zero\n",
    "    for label in labels_list:\n",
    "        label_probabilities.append(np.sum(array==label)/array_length)\n",
    "    return np.sum(label_probabilities*( np.ones(len(label_probabilities))-label_probabilities ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting features 1/2 - thresholds\n",
    "\n",
    "Our first goal is, given a feature, to find the best value on which to split it. Ideally, you would want to go through all the values of the feature, and find the one which maximizes the impurity decrease - at least that's *naively* what CART is supposed to do.\n",
    "\n",
    "To maximize impurity decrease means, conceptually, that the difference in the impurity of the parent data set, and the impurities of the child data sets is maximized. We'll see the specifics shortly and for now it's enough to know that this is, roughly, what we want to do. \n",
    "The problem with the above prescription is that you're supposed to find the global optimum, meaning the tree is supposed to go through *all* unique values in the dataset, and for each check the impurity decrease until it finds the optimum.\n",
    "\n",
    "**In pseudocode, the split is supposed to work like this**:\n",
    "\n",
    "```c\n",
    "\n",
    "parent_gini = get_gini_impurity(parent)\n",
    "max_gini_decrease = 0\n",
    "split_threshold = 0\n",
    "for threshold in (unique values of the feature):\n",
    "   left = list of rows with feature<=threshold\n",
    "   right = list of rows with feature>threshold\n",
    "   left_gini = get_gini_impurity(left)\n",
    "   right_gini = get_gini_impurity(right)\n",
    "   impurity_decrease = impurity_decrease(parent_gini, left_gini, right_gini)\n",
    "   if impurity_decrease>max_gini_decrease:\n",
    "       max_gini_decrease = impurity_decrease\n",
    "       split_threshold = threshold\n",
    "```\n",
    "\n",
    "In practice, **there's a couple of problems with the above approach**.\n",
    "\n",
    "* Features may have different numbers of unique values. Sure, in a nice, clean dataset \"age\" variable has about 100 unique values, and \"customer segment\", or similar categorical variables may have less than 5. However, there could be variables with as many unique values as there are rows: \"salary\", \"ATM withdrawal amount in last month\", \"current account average balance in last 3 months\", etc. Even for relatively small datasets, going through every possible value, calculating Ginis on 3 sets, and calculating impurity decrease may make your laptop quite slow (until you kill the process).\n",
    "\n",
    "* If you decide to test only *some* values, there's the question of which ones to test. If data is uniformly distributed, you can take linearly spaced samples and test on them. Most real-world data is not uniformly distributed, however, so you need to do something more intelligent. As far as I understand, scikit-learn does some sort of stochastic gradient decent in order to reach the optimal value. That sounds quite interesting, and I believe it may be the key reason for the difference in speeds between my algo and scikit-learn. I've decided to use a simple trick in order to find a good threshold value. Let me explain this approach.\n",
    "\n",
    "I've decided to **split the problem into three possible cases**. First, if there's less than 100 unique values, the algo should run through all of them and find the global optimum. Second, if there's between 100 and 1000 unique values, it should go through only 100 values. Finally, if there's more than 1000 values, presumably hundreds of thousands at this point, there are microscopic differences between most values, and it doesn't make sense to explore them all, so we restrict ourselves to only testing 1000 values.\n",
    "\n",
    "The second question is, what 100 or 1000 values do we use in the latter two cases? We already know that it makes no sense to split uniformly the distance between the maximum and the minimum value of the feature. Distribution of points may be a Gaussian, an exp distribution or a log-normal for example. What we want to do is to take values which are *uniformly spaced in the distribution space*, or more precisely, we want to take uniformly spaced quantiles.\n",
    "\n",
    "**The following routine performs the above logic, and then if there are more than 100 or more than 1000 unique values in the list, it takes 100 or 1000 quantiles on which the data will be split and Gini impurities computed.** Have a look at the code, with some additional explanations given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_threshold_values(array):\n",
    "    '''\n",
    "    Scans values of the array [pd.Series] and returns threshold values to be used by the decision tree to compute Gini impurity.\n",
    "    It uses different logic depending on the array length.\n",
    "    '''\n",
    "    unique_values = array.drop_duplicates().sort_values()#we want to have unique values sorted for later\n",
    "    if len(unique_values)<100:\n",
    "        return list(unique_values) #if there are less than 100 unique values that the variable takes, use all of them as thresholds\n",
    "    else:\n",
    "        #if there are up to 1000 unique values use 100 thresholds, otherwise use 1000 thresholds\n",
    "        number_of_thresholds=100 if len(unique_values)<1000 else 1000 \n",
    "        quantiles = np.linspace(0,1, number_of_thresholds) # which quantiles we want to extract\n",
    "        values = []\n",
    "        for i in quantiles[:-1]:\n",
    "            values.append(unique_values.iloc[int(i*len(unique_values))]) #add i-th quantile to the list of quantile values\n",
    "        values.append(unique_values.iloc[-1]) #the last element isn't handled properly by the linspace trick above\n",
    "        return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've explained in Part 1 that one of the key things for me was for this code to be simple. One of the ways I achieve this is by relying on some *pandas* functions, so the above code actually works with *pandas Series*, not with numpy arrays.\n",
    "\n",
    "I've also considered returning numpy array instead of a regular list from the above function, but it has almost no effect on the speed of the rest of the code.\n",
    "\n",
    "So to conclude, the above code will take an array, your feature column, with all values of the feature that appear in your dataset, it will extract unique values, see how many there are, and then return a subset of the values which are \"equally\" spaced in the sense that if a histogram of your data was not uniform but had a particular shape - there would be many more values sampled from the \"high\" parts of the histogram than from the \"low\" parts of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting features 2/2 - let the splits begin\n",
    "\n",
    "So far we've prepared two functions: ```get_gini_impurity``` that returns Gini impurity coefficient of an array of values, and ```get_threshold_values``` which selects a subset of values on which to calculate Gini impurity.\n",
    "\n",
    "Next up is the function ```split_feature``` which we will use to finally split a single feature on the value that (approximately) maximizes Gini impurity decrease. Let me guide you through the function so that you can understand the code more easily:\n",
    "\n",
    "* Parameters are explained in the function header. Let me just say that *parent* is a *pandas Dataframe* with multiple columns. Strings *feature* and *target* are column names of the feature that we're splitting and the target variable. Target variable may take different values, and all unique possibilities are listed in *target_values* parameter.\n",
    "* The code first get all the threshold values on which the split will be attempted. It then gets parent Gini impurity, dataset size, and numpy arrays of feature and target values that will be used in splitting conditions.\n",
    "* Next, we enter the loop over all thresholds\n",
    "    * Indices of two child nodes are defined\n",
    "    * Gini's of child nodes are computed\n",
    "    * Gini decrease is calculated (see more about this step below)\n",
    "* max Gini is determined and the threshold value is identified\n",
    "* feature name, max Gini impurity decrease, and corresponding threshold are returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_feature(parent, feature, target, target_values):\n",
    "    '''\n",
    "    Splits the parent dataset on the given feature by seeking to maximize Gini impurity decrease.\n",
    "    parent [dataframe] is the parent list in the tree.\n",
    "    feature [string] is the feature along which the parent list should be split by maximizing Gini impurity decrease.\n",
    "    target [string] is the column name of the target variable in the parent dataframe\n",
    "    target_values [pd.Series] is a list of values that the target takes.\n",
    "    '''\n",
    "    value_range = get_threshold_values( parent[feature] ) #get all distinct values for a given feature\n",
    "    # get parent information\n",
    "    parent_gini = get_gini_impurity(np.array(parent[target]), target_values) #gini impurity of the parent leaf\n",
    "    parent_len = len(parent) #size of parent array\n",
    "    # prepare some auxiliary lists\n",
    "    parent_feature = np.array(parent[feature]) # list of parent's values for the current feature\n",
    "    parent_label = np.array(parent[target]) # list of parent's labels \n",
    "    gini_decrease = [] # list of gini decreases for every choice of threshold\n",
    "    # split on the given feature by running through all values\n",
    "    for threshold in value_range:\n",
    "        #get the left and the right list in the binary tree\n",
    "        true_list = parent_feature>=threshold\n",
    "        false_list = parent_feature<threshold\n",
    "        #compute gini impurity for each new list\n",
    "        true_gini = get_gini_impurity(parent_label[true_list], target_values)\n",
    "        false_gini = get_gini_impurity(parent_label[false_list], target_values)\n",
    "        # get gini impurity decrease\n",
    "        gini_decrease_value = parent_gini - np.sum(true_list)/parent_len*true_gini \\\n",
    "                             - np.sum(false_list)/parent_len*false_gini\n",
    "        gini_decrease.append( gini_decrease_value ) # append gini to the list (could be a nan)\n",
    "    #get maximum decrease of gini, and the corresponding threshold\n",
    "    max_gini = max(np.nan_to_num(gini_decrease)) #turn all nan's to 0 before looking for max\n",
    "    threshold = value_range[ gini_decrease.index(max_gini) ] #threshold value at the index for which the maximum gini decrease occurs\n",
    "    return feature, max_gini, threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I admit, there's some illogical things here, like having a list of all Gini's instead of just keeping track of the maximum one, mixing between *pandas* and *numpy* a lot and so on.\n",
    "As I said already, I wrote the code going bottom-up, I was exploring and testing various things in the process, and as soon as I completed one part I would move to the next one, not going back to see what things are superfluous in the retrospect. \n",
    "What you're seeing is a first prototype, and definitely all of its parts could be put together more tightly and consistently. What I'm sharing here is a first prototype and the code is far from optimized. \n",
    "I'm aware there are many obvious, simple things that could make it cleaner.\n",
    "\n",
    "A quick note on the **computation of Gini impurity decrease**. The decrease is computed in the final lines of the loop, namely:\n",
    "\n",
    "```python\n",
    "    gini_decrease_value = parent_gini - np.sum(true_list)/parent_len*true_gini \\\n",
    "                          - np.sum(false_list)/parent_len*false_gini\n",
    "```\n",
    "\n",
    "This formula doesn't subtract child nodes' Ginis from the parent Gini, but it first multiplies them by their array lengths. Let Ginis be $G_i$ and let array lengths be $l_i$ for $i=p,1,2$, where p stands for parent, and 1,2 label the two children nodes. The above formula reads $gini\\ decrease = G_p - \\frac{l_1}{l_p}G_1 - \\frac{l_2}{l_p}G_2$. This is quite analogous to simply maximizing $l_p G_p - l_1G_1 - l_2G_2$, but it doesn't grow with the array size - it is normalized.\n",
    "\n",
    "Why not simply subtract Gini indices? Consider the following example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gini_impurity(np.array([1,1,0,0]), [1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know that the Gini of an array with two unique elements, each of which appears equal number of times, equals 1/2. The best way to split the above list, $[1,1,0,0]$, would be into lists $[1,1]$ and $[0,0]$, each of which has Gini index 1, for the Gini impurity decrease of 1-0-0=1, any way you calculate it (for any choice of normalization).\n",
    "\n",
    "The worst way to split the above list would be into two identical lists $[1,0]$. Each of those again has the Gini index of 0.5 (since it is a list of two unique elements each of which appears equal number of times).\n",
    "With no normalization, Gini impurity decrease woulw read: $G_p-G_1-G_2 = 0.5 - 0.5 - 0.5 = -0.5$ This is quite meaningless. Not only did we get a negative number, we got two lists with the same Gini as the parent list - a fact that's not obvious at all from the numerical result.\n",
    "\n",
    "With the standard normalization, $\\frac{l_i}{l_p}$, we would have obtained $0.5-0.5\\times0.5-0.5\\times0.5=0.5-0.5=0$. This looks much more natural. The worst possible result you could have is mapped to Gini decrease of 0.\n",
    "\n",
    "That's it. The Gini decrease formula subtracts child Ginis from the parent Gini, and each child Gini has a weight that's equal to its size compared to the parent.\n",
    "\n",
    "As a side note, if you were wondering, yes, in a tree you can sum Gini impurity decreases down a decision path (down a binary tree) to get the total Gini impurity decrease from the root to a leaf.\n",
    "\n",
    "Let us now move to the final piece of code in this part of the series that will knit together all of the stuff that we've seen so far.\n",
    "\n",
    "### Splitting nodes\n",
    "\n",
    "Alright, let me run through this one quickly since it is mostly a no-brainer. **We want to test all features in the dataset, computing the maximum Gini decrease for each, and then select the feature which provides us with the optimal split.**\n",
    "\n",
    "Initially you don't know on which feature to split, so there's some initialization of the splitting feature, the maximum Gini coefficient and the list of features through which we want to run.\n",
    "\n",
    "The core of the function is a loop which runs through all features, and for each one calls on ```split_feature``` to provide it with the maximum Gini impurity decrease and the corresponding threshold value for that feature.\n",
    "\n",
    "In case the new Gini impurity decrease is larger than the maximum one we've seen so far, we take the new feature as the splitting feature, and also memorize the corresponding Gini coefficient and the splitting threshold. Finally, the code returns the optimal splitting feature column name, the threshold, and the Gini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_node(dataset, target_lbl, target_vals):\n",
    "    '''\n",
    "    The function splits a node of the decision tree by choosing the feature on which to make a split,\\\n",
    "    and by choosing the value of the feature for which to make the split.\n",
    "    dataset[pd.DataFrame] is a pandas dataframe containing predictive variables (features) and target variable\n",
    "    target_lbl [string] is a name of the target variable column in the dataset \n",
    "    The function returns splitting_feature, a feature on which the split is to be made, splitting_threshold, the value\\\n",
    "    for which the split is to be made, and max_gini, the Gini impurity decrease at the split.\n",
    "    '''\n",
    "    # set up default values for the feature on which the split will happen, gini impurity decrease, and the threshold\n",
    "    splitting_feature = 'none'\n",
    "    max_gini = -1\n",
    "    splitting_threshold = 0\n",
    "    feature_list = dataset.columns[dataset.columns!=target_lbl] # get the list of all features\n",
    "    # find the feature that maximizes Gini impurity decrease\n",
    "    for feature in feature_list:\n",
    "        temp_feature, temp_gini, temp_threshold = split_feature(dataset, feature, target_lbl, target_vals)\n",
    "        # if new best case was obtained, save it\n",
    "        if temp_gini > max_gini:\n",
    "            splitting_feature = temp_feature\n",
    "            max_gini = temp_gini\n",
    "            splitting_threshold = temp_threshold\n",
    "    #return information about the optimal split\n",
    "    return [splitting_feature, splitting_threshold, max_gini]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion and discussion\n",
    "\n",
    "I've shown you how to find the (approximately) optimal threshold on which to split each feature, and how to find the optimal feature by comparing the best result obtained for each one.\n",
    "\n",
    "**I've already discussed how unoptimized the above code is, but at the same time I can say that it can run through a dataset of about one million records with 10 variables, at least half of which have almost all values unique, in a couple of seconds at most.** We can use this as a building block for the tree, and a 3, 4 level tree can be built on such a dataset in 30, 40 seconds. This *is* much slower than scikit-learn, but it is not unbearable, and with some quick fixes that I suggested it could become a bit quicker.\n",
    "\n",
    "I've timed different parts of the above code, and I found out that the slowest thing by far is ```split_feature```, and in particular the loop ``` for threshold in value_range ```. That's because there are hundredths of seconds needed to get Ginis and compute impurity decrease for every threshold value, and with 1000 values tested it's still a lot. You spend more than a second on one feature alone (for a single node). These things add up. **My best suggestion at the moment for speeding this whole thing up is to avoid going through 1000 values and to instead do some sort of a stochastic gradient descent to approximate the global optimum.** This may bring the number of tested values from 1000 to well below 100, and significantly speed up the training process.\n",
    "\n",
    "The next part is the last one. In it, I will build a binary tree of a sort which will implement everything that we've built so far to produce an actual decision tree. Thanks for reading!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
